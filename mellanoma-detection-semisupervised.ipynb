{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3376422,"sourceType":"datasetVersion","datasetId":2035877}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-06T17:09:54.332857Z","iopub.execute_input":"2024-02-06T17:09:54.333725Z","iopub.status.idle":"2024-02-06T17:09:54.339766Z","shell.execute_reply.started":"2024-02-06T17:09:54.333686Z","shell.execute_reply":"2024-02-06T17:09:54.338535Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import os\nfrom glob import glob\nimport cv2\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import layers, models\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-02-06T18:13:57.600351Z","iopub.execute_input":"2024-02-06T18:13:57.601131Z","iopub.status.idle":"2024-02-06T18:13:57.608843Z","shell.execute_reply.started":"2024-02-06T18:13:57.601081Z","shell.execute_reply":"2024-02-06T18:13:57.607198Z"},"trusted":true},"execution_count":211,"outputs":[]},{"cell_type":"code","source":"train_data = '/kaggle/input/melanoma-skin-cancer-dataset-of-10000-images/melanoma_cancer_dataset/train'","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:21:27.508172Z","iopub.execute_input":"2024-02-06T17:21:27.508614Z","iopub.status.idle":"2024-02-06T17:21:27.515279Z","shell.execute_reply.started":"2024-02-06T17:21:27.508581Z","shell.execute_reply":"2024-02-06T17:21:27.513807Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess the images\ndef load_and_preprocess_images(file_paths, target_size=(224, 224)):\n    images = [cv2.resize(cv2.imread(file_path), target_size) for file_path in file_paths]\n    images = np.array(images) / 255.0  # Normalize pixel values to the range [0, 1]\n    return images\n# Load labeled data with resizing to (100, 100)\nX_labeled = load_and_preprocess_images(labeled_files, target_size=(100, 100))\nbatch_images_resized = tf.image.resize(batch_images, (100, 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:51:19.969567Z","iopub.execute_input":"2024-02-06T17:51:19.969929Z","iopub.status.idle":"2024-02-06T17:51:19.982931Z","shell.execute_reply.started":"2024-02-06T17:51:19.969899Z","shell.execute_reply":"2024-02-06T17:51:19.981840Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"# Read and show train images\ntrain_images = []\ntrain_labels = []\nclass_mapping = {'benign': 0, 'malignant': 1}","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:50:46.738103Z","iopub.execute_input":"2024-02-06T17:50:46.738533Z","iopub.status.idle":"2024-02-06T17:50:46.759469Z","shell.execute_reply.started":"2024-02-06T17:50:46.738499Z","shell.execute_reply":"2024-02-06T17:50:46.758101Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"for folder in os.listdir(train_data):\n    data = gb.glob(os.path.join(train_data, folder, '*.jpg'))\n    print(f'{len(data)} in folder {folder}')\n    \n    # Read train images\n    for file_path in data:\n        image = cv2.imread(file_path)\n        image_array = cv2.resize(image, (100, 100))\n        train_images.append(image_array)\n        train_labels.append(class_mapping[folder])\n\n# Convert data to arrays\nX_labeled = np.array(train_images)\ny_labeled = np.array(train_labels)\n\n# Use LabelEncoder for encoding labels\nlabel_encoder = LabelEncoder()\ny_labeled_encoded = label_encoder.fit_transform(y_labeled)\n\nprint(\"X_labeled shape:\", X_labeled.shape)\nprint(\"y_labeled_encoded shape:\", y_labeled_encoded.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:52:23.178108Z","iopub.execute_input":"2024-02-06T17:52:23.178539Z","iopub.status.idle":"2024-02-06T17:52:45.457816Z","shell.execute_reply.started":"2024-02-06T17:52:23.178505Z","shell.execute_reply":"2024-02-06T17:52:45.456406Z"},"trusted":true},"execution_count":176,"outputs":[{"name":"stdout","text":"5000 in folder benign\n4605 in folder malignant\nX_labeled shape: (19210, 100, 100, 3)\ny_labeled_encoded shape: (19210,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load and preprocess the images with resizing to (224, 224)\ndef load_and_preprocess_images(file_paths, target_size=(100, 100)):\n    images = [cv2.resize(cv2.imread(file_path), target_size) for file_path in file_paths]\n    images = np.array(images) / 255.0  # Normalize pixel values to the range [0, 1]\n    return images\n\n# Step 1: Load labeled data\nlabeled_files = glob(os.path.join(train_data, '*.jpg'))\n\n# Load labeled data with resizing\nX_labeled = load_and_preprocess_images(labeled_files)\n\n# Use pseudo-labels as labels for training\nlabel_encoder = LabelEncoder()\ny_labeled_encoded = label_encoder.fit_transform([os.path.basename(file) for file in labeled_files])","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:51:40.121559Z","iopub.execute_input":"2024-02-06T17:51:40.122006Z","iopub.status.idle":"2024-02-06T17:51:40.131562Z","shell.execute_reply.started":"2024-02-06T17:51:40.121971Z","shell.execute_reply":"2024-02-06T17:51:40.129825Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"# Step 3: Load MobileNetV2 as a pre-trained backbone\n# Create the MobileNetV2 model with input shape (224, 224, 3)\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model.trainable = False  # Freeze the weights of the pre-trained model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:53:04.928516Z","iopub.execute_input":"2024-02-06T17:53:04.929766Z","iopub.status.idle":"2024-02-06T17:53:06.625283Z","shell.execute_reply.started":"2024-02-06T17:53:04.929721Z","shell.execute_reply":"2024-02-06T17:53:06.623940Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"# Step 4: Create a custom contrastive head\nembedding_size = 64\ncontrastive_head = models.Sequential([\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(embedding_size, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(embedding_size, activation='relu')\n])","metadata":{"execution":{"iopub.status.busy":"2024-02-06T17:53:08.676496Z","iopub.execute_input":"2024-02-06T17:53:08.676975Z","iopub.status.idle":"2024-02-06T17:53:08.693857Z","shell.execute_reply.started":"2024-02-06T17:53:08.676940Z","shell.execute_reply":"2024-02-06T17:53:08.692700Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"# Step 5: Build the full model with both the backbone and the custom head\ninput_labeled = layers.Input(shape=(224, 224, 3))\noutput_labeled = contrastive_head(base_model(input_labeled))\nnum_classes = 2\nflat = layers.Flatten()(base_model.output)\noutput = layers.Dense(num_classes, activation='softmax')(flat)\n\nfull_model = models.Model(inputs=input_labeled, outputs=output_labeled)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T18:09:05.560980Z","iopub.execute_input":"2024-02-06T18:09:05.561473Z","iopub.status.idle":"2024-02-06T18:09:06.224191Z","shell.execute_reply.started":"2024-02-06T18:09:05.561440Z","shell.execute_reply":"2024-02-06T18:09:06.222925Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"# Step 6: Compile the model with an appropriate optimizer and loss function\noptimizer = tf.keras.optimizers.Adam()\nfull_model.compile(optimizer=optimizer, loss='categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2024-02-06T18:09:27.405472Z","iopub.execute_input":"2024-02-06T18:09:27.405970Z","iopub.status.idle":"2024-02-06T18:09:27.429338Z","shell.execute_reply.started":"2024-02-06T18:09:27.405933Z","shell.execute_reply":"2024-02-06T18:09:27.427741Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"print(\"X_labeled shape:\", X_labeled.shape)\nprint(\"y_labeled_encoded shape:\", y_labeled_encoded.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T18:09:45.108630Z","iopub.execute_input":"2024-02-06T18:09:45.109049Z","iopub.status.idle":"2024-02-06T18:09:45.115917Z","shell.execute_reply.started":"2024-02-06T18:09:45.109006Z","shell.execute_reply":"2024-02-06T18:09:45.114613Z"},"trusted":true},"execution_count":206,"outputs":[{"name":"stdout","text":"X_labeled shape: (19210, 100, 100, 3)\ny_labeled_encoded shape: (19210,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the train_step function\n@tf.function\ndef train_step(images, labels):\n    images_resized = tf.image.resize(images, (224, 224))  # Resize images to match model input shape\n    with tf.GradientTape() as tape:\n        # Forward pass\n        logits = full_model(images_resized, training=True)\n        # Compute the loss value using categorical crossentropy\n        loss = tf.keras.losses.categorical_crossentropy(labels, logits)\n\n    # Compute gradients\n    gradients = tape.gradient(loss, full_model.trainable_variables)\n    # Update weights\n    optimizer.apply_gradients(zip(gradients, full_model.trainable_variables))\n\n    print(loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T18:09:52.724330Z","iopub.execute_input":"2024-02-06T18:09:52.725341Z","iopub.status.idle":"2024-02-06T18:09:52.734870Z","shell.execute_reply.started":"2024-02-06T18:09:52.725300Z","shell.execute_reply":"2024-02-06T18:09:52.733220Z"},"trusted":true},"execution_count":208,"outputs":[]},{"cell_type":"code","source":"# Step 9: Train the model on labeled data\nepochs = 10\nbatch_size = 32\n\n# Convert labels to one-hot encoding for categorical crossentropy\ny_labeled_one_hot = tf.keras.utils.to_categorical(y_labeled_encoded, num_classes=num_classes)\n\n# Training loop\nfor epoch in range(epochs):\n    epoch_loss = 0.0\n    for step in range(0, len(X_labeled), batch_size):\n        start_idx = step\n        end_idx = step + batch_size\n\n        batch_images = X_labeled[start_idx:end_idx]\n        batch_labels = y_labeled_one_hot[start_idx:end_idx]\n\n        # Train on the batch\n        train_step(batch_images, batch_labels)\n\n    # Optionally, calculate and print the average loss for the epoch\n    average_loss = epoch_loss / (len(X_labeled) // batch_size)\n    print(f'Epoch {epoch + 1}/{epochs}, Loss: {average_loss:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T18:10:04.644912Z","iopub.execute_input":"2024-02-06T18:10:04.645430Z","iopub.status.idle":"2024-02-06T18:10:05.778618Z","shell.execute_reply.started":"2024-02-06T18:10:04.645391Z","shell.execute_reply":"2024-02-06T18:10:05.776920Z"},"trusted":true},"execution_count":209,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[209], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     batch_labels \u001b[38;5;241m=\u001b[39m y_labeled_one_hot[start_idx:end_idx]\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Train on the batch\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Optionally, calculate and print the average loss for the epoch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(X_labeled) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_filei5dc1st_.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(images, labels)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(full_model), (ag__\u001b[38;5;241m.\u001b[39mld(images_resized),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_crossentropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(full_model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(full_model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/losses.py:2221\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m label_smoothing) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   2214\u001b[0m         label_smoothing \u001b[38;5;241m/\u001b[39m num_classes\n\u001b[1;32m   2215\u001b[0m     )\n\u001b[1;32m   2217\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39msmart_cond\u001b[38;5;241m.\u001b[39msmart_cond(\n\u001b[1;32m   2218\u001b[0m     label_smoothing, _smooth_labels, \u001b[38;5;28;01mlambda\u001b[39;00m: y_true\n\u001b[1;32m   2219\u001b[0m )\n\u001b[0;32m-> 2221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend.py:5573\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   5571\u001b[0m target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(target)\n\u001b[1;32m   5572\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m-> 5573\u001b[0m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5575\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[1;32m   5576\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5577\u001b[0m )\n\u001b[1;32m   5578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_125/352571257.py\", line 9, in train_step  *\n        loss = tf.keras.losses.categorical_crossentropy(labels, logits)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 2) and (32, 64) are incompatible\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/tmp/ipykernel_125/352571257.py\", line 9, in train_step  *\n        loss = tf.keras.losses.categorical_crossentropy(labels, logits)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/conda/lib/python3.10/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 2) and (32, 64) are incompatible\n","output_type":"error"}]}]}